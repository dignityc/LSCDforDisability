import pandas as pd
import numpy as np
import os
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline
import ast


path = 'datasets/keyword_filtered'
file_list = os.listdir(path)
subreddit = 'Blind'
sample_cutoff_value = 10

#merging sets into a single dataframe for a subreddit
filtered_files = [file for file in file_list if file.startswith(subreddit)]
all_df = pd.DataFrame()
for f in tqdm(filtered_files):
    df = pd.read_csv(f'datasets/keyword_filtered/{f}')
    df['keyword'] = f.split('_')[-1].split('.')[0]
    all_df = pd.concat([all_df,df], axis=0)

all_df = all_df.drop('index', axis=1)
all_df = all_df.reset_index()

word_count_dict = all_df['keyword'].value_counts().to_dict()
filtered_word_count_dict = {key: value for key, value in word_count_dict.items() if value > sample_cutoff_value}


#LSA 


for key, value in filtered_word_count_dict.items():
    key = 'HE' 
    filtered_df = all_df[all_df['keyword']==key].copy()
    filtered_df['anonymized_body_stringfied'] = filtered_df['anonymized_body_lemmatized'].apply(ast.literal_eval)
    filtered_df['anonymized_body_textualized'] = filtered_df['anonymized_body_stringfied'].apply(lambda x: ' '.join(x))
    tfidf_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
    svd_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=100, random_state=122)

    lsa_pipeline = Pipeline([
        ('tfidf', tfidf_vectorizer),
        ('svd', svd_model)
    ])

    lsa_matrix = lsa_pipeline.fit_transform(filtered_df['anonymized_body_textualized'])

    #print("LSA Topic Matrix:")
    #print(lsa_matrix)

    print("\nTopic Word Weights:")
    feature_names = tfidf_vectorizer.get_feature_names_out()
    for i, topic in enumerate(svd_model.components_):
        print(f"Topic {i}:")
        print([feature_names[index] for index in topic.argsort()[:-6:-1]])
        print(np.sort(topic)[:-6:-1])
    break

#word embedding-based simliarty
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors
import pandas as pd


model = Word2Vec(vector_size=100, min_count=1)
model.build_vocab(df['anonymized_body_lemmatized'])
model.train(df['anonymized_body_lemmatized'], total_examples=model.corpus_count, epochs=model.epochs)

word1 = 'dumb'
similarities = {word: model.wv.similarity(word1, word) 
                for word in model.wv.index_to_key if word != word1}

sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)

N = 10  
for word, similarity in sorted_similarities[:N]:
    print(f"Similarity between '{word1}' and '{word}': {similarity:.4f}")