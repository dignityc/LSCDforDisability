{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8920b77d",
   "metadata": {},
   "source": [
    "Here, I reproduce the method of Jessica Lin (which is the best) for all the different KGs considered in my experiments in order to quantify the impact of each type of knowledge with respect to the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873f0465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\diboc\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.22.4)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: nltk in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.21.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.11.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\diboc\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (8.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d71b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "mlp = MLPClassifier(solver='adam', alpha=0.0001, hidden_layer_sizes=(512, 512, 512), max_iter=500, learning_rate=\"constant\", n_iter_no_change=100,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9ebc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('datasets/hatexplain_processed_kgs.xlsx')\n",
    "df['Wikipedia_description'].fillna('', inplace=True)\n",
    "df['Wikidata_description'].fillna('', inplace=True)\n",
    "df['ConceptNet_terms'].fillna('', inplace=True)\n",
    "df['KnowledJe_context'].fillna('', inplace=True)\n",
    "df['context'] = df['Wikipedia_description'] + df['Wikidata_description'] + df['ConceptNet_terms'] + df['KnowledJe_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7eb501b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>split_set</th>\n",
       "      <th>class</th>\n",
       "      <th>target</th>\n",
       "      <th>explanation</th>\n",
       "      <th>Wikipedia_description</th>\n",
       "      <th>Wikidata_description</th>\n",
       "      <th>ConceptNet_terms</th>\n",
       "      <th>KnowledJe_context</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23107796_gab</td>\n",
       "      <td>u really think i would not have been raped by ...</td>\n",
       "      <td>train</td>\n",
       "      <td>offensive</td>\n",
       "      <td>['Hindu']</td>\n",
       "      <td>['raped', 'feral', 'hindu', 'muslim', 'neo', '...</td>\n",
       "      <td>'''Muslims''' are people who follow or practic...</td>\n",
       "      <td>adherents of the religion of Islam</td>\n",
       "      <td>uranium uracil remember consider intend cerebr...</td>\n",
       "      <td></td>\n",
       "      <td>'''Muslims''' are people who follow or practic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9995600_gab</td>\n",
       "      <td>the uk has threatened to return radioactive wa...</td>\n",
       "      <td>train</td>\n",
       "      <td>offensive</td>\n",
       "      <td>['Refugee']</td>\n",
       "      <td>['send', 'back', 'all', 'the', 'paki', 'migran...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>threaten endanger proceeds reelect return key ...</td>\n",
       "      <td></td>\n",
       "      <td>threaten endanger proceeds reelect return key ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1227920812235051008_twitter</td>\n",
       "      <td>if english is not imposition then hindi is als...</td>\n",
       "      <td>train</td>\n",
       "      <td>offensive</td>\n",
       "      <td>['Other']</td>\n",
       "      <td>['chutiya', 'retards', 'stophindiimposition']</td>\n",
       "      <td>'''English''' is a Germanic language of the In...</td>\n",
       "      <td>West Germanic language</td>\n",
       "      <td>infliction hindu closed close unopen exclude r...</td>\n",
       "      <td></td>\n",
       "      <td>'''English''' is a Germanic language of the In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1204931715778543624_twitter</td>\n",
       "      <td>no liberal congratulated hindu refugees post c...</td>\n",
       "      <td>train</td>\n",
       "      <td>offensive</td>\n",
       "      <td>['Hindu']</td>\n",
       "      <td>['hate', 'hindus']</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>bighearted loose progressive large-minded free...</td>\n",
       "      <td></td>\n",
       "      <td>bighearted loose progressive large-minded free...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179102559241244672_twitter</td>\n",
       "      <td>he said bro even your texts sound redneck what...</td>\n",
       "      <td>train</td>\n",
       "      <td>offensive</td>\n",
       "      <td>['Economic']</td>\n",
       "      <td>['redneck']</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ID  \\\n",
       "0                 23107796_gab   \n",
       "1                  9995600_gab   \n",
       "2  1227920812235051008_twitter   \n",
       "3  1204931715778543624_twitter   \n",
       "4  1179102559241244672_twitter   \n",
       "\n",
       "                                        cleaned_text split_set      class  \\\n",
       "0  u really think i would not have been raped by ...     train  offensive   \n",
       "1  the uk has threatened to return radioactive wa...     train  offensive   \n",
       "2  if english is not imposition then hindi is als...     train  offensive   \n",
       "3  no liberal congratulated hindu refugees post c...     train  offensive   \n",
       "4  he said bro even your texts sound redneck what...     train  offensive   \n",
       "\n",
       "         target                                        explanation  \\\n",
       "0     ['Hindu']  ['raped', 'feral', 'hindu', 'muslim', 'neo', '...   \n",
       "1   ['Refugee']  ['send', 'back', 'all', 'the', 'paki', 'migran...   \n",
       "2     ['Other']      ['chutiya', 'retards', 'stophindiimposition']   \n",
       "3     ['Hindu']                                 ['hate', 'hindus']   \n",
       "4  ['Economic']                                        ['redneck']   \n",
       "\n",
       "                               Wikipedia_description  \\\n",
       "0  '''Muslims''' are people who follow or practic...   \n",
       "1                                                      \n",
       "2  '''English''' is a Germanic language of the In...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                 Wikidata_description  \\\n",
       "0  adherents of the religion of Islam   \n",
       "1                                       \n",
       "2              West Germanic language   \n",
       "3                                       \n",
       "4                                       \n",
       "\n",
       "                                    ConceptNet_terms KnowledJe_context  \\\n",
       "0  uranium uracil remember consider intend cerebr...                     \n",
       "1  threaten endanger proceeds reelect return key ...                     \n",
       "2  infliction hindu closed close unopen exclude r...                     \n",
       "3  bighearted loose progressive large-minded free...                     \n",
       "4                                                                        \n",
       "\n",
       "                                             context  \n",
       "0  '''Muslims''' are people who follow or practic...  \n",
       "1  threaten endanger proceeds reelect return key ...  \n",
       "2  '''English''' is a Germanic language of the In...  \n",
       "3  bighearted loose progressive large-minded free...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70d47fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19229,)\n"
     ]
    }
   ],
   "source": [
    "combined_bk = df['cleaned_text']+df['KnowledJe_context']\n",
    "combined_bk = combined_bk.tolist()\n",
    "\n",
    "df['class'].replace({'hate_speech':0, 'neutral':1, 'offensive':2}, inplace=True) # change for other dataset's labels\n",
    "labels=df['class'].tolist()\n",
    "labels=np.array(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66434487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence BERT embeddings\n",
    "\n",
    "combined_bk_emb = model.encode(combined_bk)\n",
    "np.save('embeddings/hatexplain/sentenceBERT_KnowledJe.npy', combined_bk_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for each set\n",
    "train_indices = np.where(df['split_set'] != 'test')[0]\n",
    "test_indices = np.where(df['split_set'] == 'test')[0]\n",
    "\n",
    "# Split document embeddings based on indices\n",
    "train_embeddings = combined_bk_emb[train_indices]\n",
    "test_embeddings = combined_bk_emb[test_indices]\n",
    "\n",
    "assert len(train_embeddings) == (df.split_set.value_counts()['train']+df.split_set.value_counts()['val'])\n",
    "assert len(test_embeddings) == df.split_set.value_counts()['test']\n",
    "\n",
    "# get labels\n",
    "train_labels = labels[train_indices]\n",
    "test_labels = labels[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(train_embeddings, train_labels)\n",
    "score=mlp.score(test_embeddings, test_labels) #acc\n",
    "bin_preds=mlp.predict(test_embeddings)\n",
    "\n",
    "print(classification_report(test_labels, bin_preds, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
